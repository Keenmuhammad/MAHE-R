{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Keenmuhammad/MAHE-R/blob/main/Webinar6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Load essential libraries\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "0_BbyTKXQflD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler"
      ],
      "metadata": {
        "id": "20W0d4ruQjE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Demonstration of splitting samples into batches for batch processing using a simple example\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "4PA3YCYjerWB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtKh7DZWkCMa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d51fac2d-a27a-493f-f6d2-b15e907bca68"
      },
      "source": [
        "num_samples = 11 # total number of samples\n",
        "num_iters = 10   # number of iterations\n",
        "batch_size = 16   # number of samples for calculating loss and gradient in each iteration\n",
        "\n",
        "print('Number of samples = %d'%(num_samples))\n",
        "print('Number of iterations = %d'%(num_iters))\n",
        "print('Batch size = %d'%(batch_size))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples = 11\n",
            "Number of iterations = 10\n",
            "Batch size = 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "User-defined function to generate sample indices for batch processing according to batch size\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "BzkJAqOaevwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.arange(3, 10, 2)\n",
        "np.random.choice([10, 20, 30, 40, 50], 3, replace = True, p = [0.1, 0.1, 0.2, 0.2, 0.4])\n",
        "myarray = np.arange(10)\n",
        "print(myarray)\n",
        "chunks = np.array_split(myarray, 6)\n",
        "chunks = np.array_split(myarray, [3, 6, 9])\n",
        "print(chunks)\n",
        "print(type(chunks))\n",
        "#print(chunks.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWzX7yv3xHaR",
        "outputId": "72480ec9-b9a5-481e-c287-3957791c1031"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 2 3 4 5 6 7 8 9]\n",
            "[array([0, 1, 2]), array([3, 4, 5]), array([6, 7, 8]), array([9])]\n",
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reordered_sample_indices = np.random.choice(num_samples, num_samples, replace = False)\n",
        "print(reordered_sample_indices)\n",
        "print(np.arange(batch_size, num_samples, batch_size))\n",
        "np.array_split(reordered_sample_indices, np.arange(batch_size, len(reordered_sample_indices), batch_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3US2LpA1FNx",
        "outputId": "16209013-14d2-49eb-d9f3-9c57a3015e06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 6  3  5  1  4 10  2  9  0  7  8]\n",
            "[3 6 9]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([6, 3, 5]), array([ 1,  4, 10]), array([2, 9, 0]), array([7, 8])]"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_batch_indices():\n",
        "  # Reorder sample indices\n",
        "  reordered_sample_indices = np.random.choice(num_samples, num_samples, replace = False)\n",
        "  # Generate batch indices for batch processing\n",
        "  batch_indices = np.array_split(reordered_sample_indices, np.arange(batch_size, len(reordered_sample_indices), batch_size))\n",
        "  return(batch_indices)"
      ],
      "metadata": {
        "id": "ks4Yx0HNQ4sU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_batch_indices()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbHP0rtn6nx_",
        "outputId": "81c107fd-4536-4831-be58-9ab593937c7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([1, 7, 0]), array([6, 2, 9]), array([8, 4, 3]), array([10,  5])]"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "The batch processiong loop\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "dqLN2H2HVl9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of batches per epoch\n",
        "num_iterations_per_epoch = int(np.ceil(num_samples/batch_size)) \n",
        "print('Number of iterations per epoch = %d\\n'%(num_iterations_per_epoch))\n",
        "b = 0\n",
        "epoch = 0\n",
        "for it in range(num_iters):\n",
        "  if it % num_iterations_per_epoch == 0:# check if we are at the start of an epoch\n",
        "    print('--------------------------------')\n",
        "    print('Epoch %d:'%(epoch+1))\n",
        "    batch_indices = generate_batch_indices()\n",
        "    b = 0 \n",
        "    epoch = epoch + 1   \n",
        "    print('--------------------------------')\n",
        "  print('In iteration %d, using samples' % (it+1))\n",
        "  print(batch_indices[b])  \n",
        "  b += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTAv0Ci2VcA4",
        "outputId": "39554a75-75ab-455c-da3f-7d1e141ef96b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of iterations per epoch = 4\n",
            "\n",
            "--------------------------------\n",
            "Epoch 1:\n",
            "--------------------------------\n",
            "In iteration 1, using samples\n",
            "[ 4 10  2]\n",
            "In iteration 2, using samples\n",
            "[5 9 7]\n",
            "In iteration 3, using samples\n",
            "[0 3 8]\n",
            "In iteration 4, using samples\n",
            "[1 6]\n",
            "--------------------------------\n",
            "Epoch 2:\n",
            "--------------------------------\n",
            "In iteration 5, using samples\n",
            "[ 9 10  7]\n",
            "In iteration 6, using samples\n",
            "[1 0 3]\n",
            "In iteration 7, using samples\n",
            "[8 4 5]\n",
            "In iteration 8, using samples\n",
            "[6 2]\n",
            "--------------------------------\n",
            "Epoch 3:\n",
            "--------------------------------\n",
            "In iteration 9, using samples\n",
            "[6 9 4]\n",
            "In iteration 10, using samples\n",
            "[3 8 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Generate artificial data with 11 samples, 2 features per sample and 3 output classes.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "jAYpx802XT1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 11 # number of samples\n",
        "num_features = 2 # number of features (a.k.a. dimensionality)\n",
        "num_labels = 3 # number of output labels\n",
        "\n",
        "# Data matrix (each column = single sample)\n",
        "X = np.random.choice(np.arange(0, 5), size = (num_features, num_samples), replace = True)\n",
        "\n",
        "# Class labels\n",
        "y = np.random.choice([0, 1, 2], size = num_samples, replace = True)\n",
        "\n",
        "print('Data matrix X = ')\n",
        "print(X)\n",
        "print('Data labels y = ')\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pf2SGwDvXUM4",
        "outputId": "aa30c1f4-d410-4e00-b85d-184c181ec152"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data matrix X = \n",
            "[[4 2 0 0 2 2 1 0 2 1 4]\n",
            " [2 0 3 3 3 0 1 4 2 3 3]]\n",
            "Data labels y = \n",
            "[0 2 2 0 1 0 0 0 1 2 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Batch gradient descent for the softmax classification algorithm\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "KTd1dynKYJgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial weights matrix\n",
        "W = 0.01 * np.random.randn(num_labels, num_features)\n",
        "\n",
        "num_iters = 10        # number of iterations\n",
        "batch_size = 3        # number of samples in each batch\n",
        "learning_rate = 1e-02 # learning rate\n",
        "\n",
        "print('Number of samples = %d'%(num_samples))\n",
        "print('Number of iterations = %d'%(num_iters))\n",
        "print('Batch size = %d'%(batch_size))\n",
        "\n",
        "## Batch Gradient descent loop\n",
        "b = 0\n",
        "epoch = 0\n",
        "num_batches_per_epoch = int(np.ceil(num_samples/batch_size)) # number of batches per epoch\n",
        "print('Number of batches per epoch = %d\\n'%(num_batches_per_epoch))\n",
        "it_loss_history = [] # initialize empty list for storing loss history over each iteration\n",
        "epoch_loss_history = [] # initialize empty list for storing loss history over each epoch\n",
        "\n",
        "for it in range(num_iters):\n",
        "  # Generate batch indices if all samples have been seen at this iteration\n",
        "  if it % num_batches_per_epoch == 0:\n",
        "    batch_indices = generate_batch_indices()\n",
        "    b = 0\n",
        "    if epoch >= 1:\n",
        "      epoch_avg_loss = np.mean(it_loss_history[it-len(batch_indices):it])\n",
        "      print('----------------------------------')\n",
        "      print('Epoch: %d, Loss = %f' % (epoch, epoch_avg_loss))  \n",
        "      print('----------------------------------')\n",
        "      epoch_loss_history.append(epoch_avg_loss) # append average epoch loss  \n",
        "    epoch = epoch + 1 \n",
        "      \n",
        "  # Evaluate loss and gradient on batch samples\n",
        "  batch_sample_size = len(batch_indices[b]) # number of samples in each batch\n",
        "  \n",
        "  # Calculate the scores matrix\n",
        "  S = np.dot(W, X[:, batch_indices[b]])\n",
        "\n",
        "  # Calculate the probability matrix\n",
        "  P = np.exp(S) / np.sum(np.exp(S), axis = 0)\n",
        "\n",
        "  # Calculate loss for all samples in current batch\n",
        "  loss = -np.log(P[y[batch_indices[b]], np.arange(batch_sample_size)])\n",
        "\n",
        "  # Calculate total average data loss for current batch\n",
        "  loss_data = np.mean(loss)\n",
        "    \n",
        "  # Adjust probability matrix such that 1 is subtracted from each batch sample's\n",
        "  # correct category probability\n",
        "  P[y[batch_indices[b]], np.arange(batch_sample_size)] -= 1\n",
        "\n",
        "  # Calculate the gradient w.r.t. weights of the data loss\n",
        "  dW = (1/batch_sample_size) * np.dot(P, X[:, batch_indices[b]].T) \n",
        "\n",
        "  # Update weights matrix\n",
        "  W = W - learning_rate * dW\n",
        "   \n",
        "  # Append iteration loss history and print loss once every iteration\n",
        "  it_loss_history.append(loss_data)\n",
        "  print('# In iteration %d, using samples' % (it+1))\n",
        "  print(batch_indices[b]) \n",
        "  print('Loss = %f'%(loss_data))\n",
        "  \n",
        "  b += 1 "
      ],
      "metadata": {
        "id": "8se9Ym9FYGN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Ws the jury selection biased?**\n",
        "\n",
        "---\n",
        "\n",
        "In 1963, a county court in Alabama, U.S.A, convicted a young African-American man named Robert Swain of a heinous crime and sentenced him to death. At the time, only men aged 21 or older were allowed to serve on juries in that county where in 26% of the eligible jurors were African-American. However, for the 100-member jury panel for the case's trial, only 8 eligible African-American men were selected. None of those selected 8 African-American men ended up being part of the actual jury panel for the trial. Mr. Swain appealed his sentence, citing among other factors the all-Caucasian jury.\n",
        "\n",
        "In 1965, the Supreme Court of the United States denied Swain’s appeal. In its ruling, the Court observed that the overall percentage disparity of African-Americans has been small and was not due to any concerted effort.\n",
        "\n",
        "The percentage disparity in this case was 8% (the percentage of African-American jurors on the jury panel) compared to 26% (the percentage of African-American people eligible for jury service). How could we decide if that quoted disparity is small?\n",
        "\n",
        "---\n",
        "\n",
        "To answer this question, we will simulate a jury pool assuming that that each juror has been randomly selected from the eligible population. That is, for any one juror, there is a probability of 26% that they are African-American.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "dmcsWxTIac7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate a 100-member jury panel\n",
        "Jury_Panel = np.random.choice(['C', 'A'], size = 100, replace = True, p = [0.74, 0.26])\n",
        "np.count_nonzero(Jury_Panel == 'A')"
      ],
      "metadata": {
        "id": "OV0BLY4Wc4K5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "We will now simualte the 100-member jury panel several times and see if it is actually reasonably likely to have a jury panel with 8 African-Americans.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "K9HMXW55fZj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nsimulations = 100\n",
        "# Make an empty counts array, to append to\n",
        "counts = np.array([], dtype = int)\n",
        "for j in range(nsimulations):\n",
        "  Jury_Panel = np.random.choice(['C', 'A'], size = 100, replace = True, p = [0.74, 0.26])\n",
        "  counts = np.append(counts, np.count_nonzero(Jury_Panel == 'A'))   "
      ],
      "metadata": {
        "id": "3cbuAJmOflkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Plot a histogram of the number of African-American jurors.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "5o9xtwa1h06x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize = (10, 6))\n",
        "fig.tight_layout(pad = 4.0)\n",
        "binwidth = 2\n",
        "ax.hist(counts, bins = range(min(counts), max(counts) + binwidth, binwidth))\n",
        "ax.set_xlabel('No. of African American Jurors', fontsize = 14)\n",
        "ax.set_ylabel('Count', fontsize = 14)\n",
        "ax.set_xticks(range(min(counts), max(counts) + binwidth, binwidth));"
      ],
      "metadata": {
        "id": "wdiIMdXEh5tC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Load the iris dataset\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "FPpQ-TBukETG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()"
      ],
      "metadata": {
        "id": "Zgr6EZXDkOEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "What is inside the 'iris' object?\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "fLoqQdChkPZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(iris)\n",
        "X = iris.data\n",
        "print(type(X))\n",
        "print(X.shape)"
      ],
      "metadata": {
        "id": "9EXq4XvxkYH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = X[:, 2]\n",
        "#print(np.sum(a)/len(a))\n",
        "print(np.mean(a))\n",
        "print(np.median(a))\n",
        "# What fraction of samples have a sepal length <= median sepal length\n",
        "#print(a)\n",
        "#print(np.mean(a >= np.median(a))*100) \n",
        "# Median is the 50th percentile\n",
        "#print(np.percentile(a, 50))\n",
        "# 90th percentile\n",
        "#print(np.percentile(a, 90))\n",
        "#print(np.mean(a <= np.percentile(a, 90))*100)\n",
        "print(np.var(a))\n",
        "np.mean((a - np.mean(a))**2)\n",
        "print(np.std(a))\n",
        "np.sqrt(np.mean((a - np.mean(a))**2))"
      ],
      "metadata": {
        "id": "_2JKQTXpkkO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Plot a histogram of the number of sepal lengths.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "KRZpglROktcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize = (10, 6))\n",
        "fig.tight_layout(pad = 4.0)\n",
        "binwidth = 2\n",
        "ax.hist(counts, bins = range(min(counts), max(counts) + binwidth, binwidth))\n",
        "ax.set_xlabel('Sepal length', fontsize = 14)\n",
        "ax.set_ylabel('Count', fontsize = 14)\n",
        "ax.set_title('Sepal Length Distribution in Iris Flower', fontsize = 16)\n",
        "ax.set_xticks(range(min(counts), max(counts) + binwidth, binwidth));"
      ],
      "metadata": {
        "id": "gkVFIFTqkxIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "User-defined function to make a component plot of an array\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "stJmyZ-flRI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plotveccomp(x, name = None, axis = None):\n",
        "  ax = axis\n",
        "  component_index = range(0, len(x))\n",
        "  ax.plot(component_index, x, color = 'black', marker = 'o') \n",
        "  ax.plot(component_index, [np.mean(x)]*len(x), linewidth = 1, linestyle = 'dashed', color ='blue') \n",
        "  ax.plot(component_index, [np.mean(x) - np.std(x)]*len(x), linewidth = 1, linestyle = 'dashed', color ='red')\n",
        "  ax.plot(component_index, [np.mean(x) + np.std(x)]*len(x), linewidth = 1, linestyle = 'dashed', color ='red')\n",
        "  ax.set_xlabel('Sample #', fontsize = 16)\n",
        "  ax.set_ylabel(name, fontsize = 16)\n",
        "  ax.set_title(' '.join(name.split()[0:2]) +  ' Array', fontsize = 14)"
      ],
      "metadata": {
        "id": "TbELW5h5lQvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax1 = plt.subplots(1, 1, figsize = (8, 8))\n",
        "fig.tight_layout(pad = 4.0)\n",
        "plotveccomp(a, 'Sepal Length', ax1)"
      ],
      "metadata": {
        "id": "y8z_dWpclguW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Mount Google Drive folder\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "imohJRqynJBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google drive folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "IbzObNaGnMPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Setup working directory and data directory.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "1xth6Akbmhjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DIR = '/content/drive/MyDrive/Colab Notebooks/MAHE/Office of Online Education/'\n",
        "DATA_DIR = DIR + 'Data/'"
      ],
      "metadata": {
        "id": "hDTbe7NKmh9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Load Data\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "5hmtna_0msSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FILE = DATA_DIR + 'food-texture.csv'\n",
        "df_food = pd.read_csv(FILE, index_col = 0)\n",
        "df_food.head()"
      ],
      "metadata": {
        "id": "1np3DDRBmw83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Print the names of the rows & columns in the dataframe\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "A5fLF76gnUVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_food.index)\n",
        "print(df_food.columns)"
      ],
      "metadata": {
        "id": "TKVJoRianafW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Get values in the 'Density' column\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "KvqycbPPnceM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_food['Density'])"
      ],
      "metadata": {
        "id": "V8GbAbG-nhA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Get features for the sample B136\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "67hIG7zknjBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_food.loc['B136', :])"
      ],
      "metadata": {
        "id": "ys-gzaWgnnSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Get 'Oil' and 'Density' values for the samples B136 and B225.\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "p6RCvOgLnoKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = ['Oil', 'Density']\n",
        "sample_names = ['B136', 'B225']\n",
        "#df_food[feature_names]\n",
        "df_food_sub = df_food.loc[sample_names, feature_names]\n",
        "print(df_food_sub)"
      ],
      "metadata": {
        "id": "J8WyHpCMnvZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Object type\n",
        "#type(df_food)\n",
        "#type(df_food['Oil'])\n",
        "#type(df_food['Oil'].values)\n",
        "#type(df_food.loc['B136', :])\n",
        "#type(df_food.loc['B136', 'Oil'])\n",
        "#df_food['Oil'].dtype\n",
        "#type(df_food[feature_names])\n",
        "#df_food[feature_names].dtypes\n",
        "df_food.dtypes"
      ],
      "metadata": {
        "id": "MtvqoLkEny7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Create a list of continuous and categorical column names and set types accordingly\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "-r6lgRwjnzsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "continuous_cols = ['Oil', 'Density', 'Fracture', 'Hardness']\n",
        "categorical_cols = ['Crispy']\n",
        "\n",
        "# Typecasting\n",
        "df_food[categorical_cols] = df_food[categorical_cols].astype('category')\n",
        "df_food[continuous_cols] = df_food[continuous_cols].astype('float64')\n",
        "\n",
        "df_food.dtypes"
      ],
      "metadata": {
        "id": "8CWeYghun8cD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Simulating a data centre-resource dataframe\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "0TSii1KXo62X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(100)\n",
        "nsamples = 10\n",
        "colnames = ['Number of Cores', 'Memory', 'Disk Space', 'Network Bandwidth']\n",
        "data = zip(np.random.choice(np.arange(2,17), nsamples),\n",
        "           np.random.choice([16, 32, 64, 128], nsamples),\n",
        "           np.random.choice([128, 256, 512, 1024], nsamples),\n",
        "           np.random.choice(100*np.arange(2, 11), nsamples))\n",
        "df_resource = pd.DataFrame(list(data), columns = colnames)\n",
        "df_resource['Power Consumption'] = np.random.normal(2, df_resource['Memory'] / (np.max(df_resource['Memory'])-np.min(df_resource['Memory'])), nsamples)\n",
        "df_resource.head()"
      ],
      "metadata": {
        "id": "_pxf40MXo9Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Handling time series data\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "pGSL7Bnyq-u1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FILE = DATA_DIR + 'testdata.csv'\n",
        "df = pd.read_csv(FILE, sep = \",\", header = 0)\n",
        "df['time'] = pd.to_datetime(df['time'], format='%m-%d-%Y %H.%M')\n",
        "df.loc[:, (df.columns != 'time')] = df.loc[:, df.columns != 'time'].apply(pd.to_numeric, errors = 'coerce')\n",
        "df = df.set_index('time')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "f4gFLUVOrBW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Plot percentage of missing values (NaNs) for each feature\n",
        "cutoff = 30\n",
        "fig = plt.figure(figsize=(6, 6))\n",
        "percent_missing = (df.isna().sum() / df.shape[0]) * 100\n",
        "percent_missing.plot(kind = 'bar', color = cm.rainbow(np.linspace(0, 1, 2))[(percent_missing <= cutoff).values.astype(int)])\n",
        "plt.plot(np.arange(df.shape[1]), np.repeat(cutoff, df.shape[1]), 'g--') \n",
        "fig.suptitle('Percentage Missing Values Across All Features', fontsize = 20)\n",
        "plt.xlabel('Feature', fontsize = 16)\n",
        "plt.ylabel('% Missing Values', fontsize = 16)"
      ],
      "metadata": {
        "id": "gUfqNpserYTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Linear interpolation for one column\n",
        "#df['Cyclone_Inlet_Gas_Temp'] = df['Cyclone_Inlet_Gas_Temp'].interpolate(method = 'linear')\n",
        "df.loc[:, (df.columns != 'time')] = df.loc[:, df.columns != 'time'].interpolate(method = 'linear')\n",
        "(df.isna().sum() / df.shape[0]) * 100"
      ],
      "metadata": {
        "id": "d0aBAaHgrmPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, FunctionTransformer"
      ],
      "metadata": {
        "id": "F8r4lAOOr_Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature = df.columns[0] # \n",
        "sampling_period = 1 # in seconds of the dataset as provided\n",
        "time_period = '10min' # for each sample\n",
        "scaler = {'identity': FunctionTransformer(lambda x: x), 'standard': StandardScaler()}\n",
        "df1 = pd.DataFrame(scaler['identity'].fit_transform(df))\n",
        "df1.index = df.index.copy()\n",
        "df1.columns = df.columns.copy()\n",
        "df_samples = df1.groupby(pd.Grouper(freq = time_period)).apply(lambda x: x[feature].values if len(x[feature].values) == int(pd.Timedelta(time_period).total_seconds()) else np.nan)\n",
        "df_samples = df_samples.dropna()\n",
        "df_samples.head()"
      ],
      "metadata": {
        "id": "1zzjy4wwrnGr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}